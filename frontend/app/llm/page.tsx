'use client'

import { Suspense, useEffect, useState } from 'react'
import ChatArea from '@/components/chat/ChatArea'
import { Brain } from 'lucide-react'
import { randomUUID, replaceEventDataToText } from '@/public/ts/commonUtil'
import { cancelStreamApi, streamApi } from '@/api/stream'
import { chatLlmApi } from '@/api/chat'
import { useUiStore } from '@/stores/uiStore'
import { useModalStore } from '@/stores/modalStore'
import { GreetingMessage } from '@/public/const/greeting'
import { createAnswerMessage, createQueryMessage, Message } from '@/types/chat'
import { StreamEvent } from '@/types/streamEvent'
import NotFound from '@/components/common/NotFound'

function LlmContent() {
  const uiStore = useUiStore()
  const modalStore = useModalStore()

  // ###################################################
  // ìƒíƒœ ê´€ë¦¬
  // ###################################################
  // ì„¸ì…˜ ID ìƒíƒœ
  const [sessionId] = useState<string>(randomUUID())
  // ëŒ€í™” ë‚´ì—­ ëª©ë¡ ìƒíƒœ
  const [messages, setMessages] = useState<Message[]>([])
  // ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€ ìƒíƒœ
  const [isStreaming, setIsStreaming] = useState(false)

  useEffect(() => {
    setMessages([createAnswerMessage('', '')])
    let greetingMessageIndex = 0
    const greetingMessageInterval = setInterval(() => {
      setMessages((prev) => {
        if (prev.length === 0) return prev
        const messages = [...prev]
        const lastIndex = 0
        messages[lastIndex] = {
          ...messages[lastIndex],
          content: replaceEventDataToText(
            GreetingMessage.llm.substring(0, greetingMessageIndex),
          ),
        }
        return messages
      })
      if (greetingMessageIndex >= GreetingMessage.llm.length) {
        clearInterval(greetingMessageInterval)
      } else {
        greetingMessageIndex++
      }
    }, 10)

    return () => clearInterval(greetingMessageInterval)
  }, [])

  // ###################################################
  // í•¸ë“¤ëŸ¬
  // ###################################################
  /**
   * ë‹µë³€ ìš”ì²­ í•¸ë“¤ëŸ¬
   * @param query ì‚¬ìš©ì ì§ˆì˜
   */
  const handleSendQuery = async (query: string) => {
    // ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ì²´í¬
    if (isStreaming) return
    // ìŠ¤íŠ¸ë¦¼ ì‹œì‘ ìƒíƒœ ë³€ê²½
    setIsStreaming(true)
    // ì„¸ì…˜ ê¸°ë°˜ SSE ì—°ê²°
    await streamApi(
      sessionId,
      new StreamEvent({
        onConnect: async (_) => {
          console.log(`ğŸ“¡ ì§ˆì˜ ìš”ì²­ : ${query}`)
          // ì§ˆì˜ ë“±ë¡
          setMessages((prev) => [...prev, createQueryMessage(query)])
          await chatLlmApi(query, sessionId)
            .then((response) => {
              console.log(`ğŸ“¡ ${response.message}`)
              // ë‹µë³€ ë“±ë¡
              setMessages((prev) => [...prev, createAnswerMessage('', '', [])])
            })
            .catch((reason) => {
              console.error(reason)
              modalStore.setInfo('ì„œë²„ í†µì‹  ì—ëŸ¬', 'ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
              setIsStreaming(false)
            })
        },
        onDisconnect: (_) => {
          setIsStreaming(false)
        },
        onException: (_) => {
          setIsStreaming(false)
        },
        onError: (_) => {
          modalStore.setInfo('ì„œë²„ í†µì‹  ì—ëŸ¬', 'ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
          setIsStreaming(false)
        },
        onInference: (event) => {
          setMessages((prev) => {
            const messages = [...prev]
            const currentMessageIndex = messages.length - 1
            const currentMessage = messages[currentMessageIndex]
            messages[currentMessageIndex] = {
              ...currentMessage,
              inference: replaceEventDataToText(
                currentMessage.inference + event.data,
              ),
            }
            return messages
          })
        },
        onAnswer: (event) => {
          setMessages((prev) => {
            const messages = [...prev]
            const currentMessageIndex = messages.length - 1
            const currentMessage = messages[currentMessageIndex]
            messages[currentMessageIndex] = {
              ...currentMessage,
              content: replaceEventDataToText(
                currentMessage.content + event.data,
              ),
            }
            return messages
          })
        },
      }),
    )
  }

  /**
   * ìŠ¤íŠ¸ë¦¼ ì¤‘ë‹¨ í•¸ë“¤ëŸ¬
   */
  const handleStop = async () => {
    await cancelStreamApi(sessionId)
      .then((response) => {
        console.log(`ğŸ“¡ ${response.message}`)
      })
      .catch((reason) => console.error(reason))
      .finally(() => setIsStreaming(false))
  }

  // ###################################################
  // ë Œë”ë§ (Render)
  // ###################################################
  return (
    <div className="flex h-full w-full flex-col p-6">
      {/* í—¤ë” ì˜ì—­ */}
      <div className="mb-6 flex items-center justify-between">
        <div className="flex items-center gap-3">
          <div>
            <h2 className="flex items-center gap-2 text-2xl font-bold text-gray-800">
              <Brain className="text-primary h-6 w-6" />
              LLM Chat
            </h2>
            <p className="mt-1 text-xs text-gray-500">ì¼ë°˜ ì§ˆë¬¸ & ë‹µë³€</p>
          </div>
        </div>
      </div>
      {/* ì±„íŒ… ì˜ì—­ */}
      <div className="min-h-0 flex-1">
        <ChatArea
          messages={messages}
          onSendMessage={handleSendQuery}
          onStop={handleStop}
          isStreaming={isStreaming}
        />
      </div>
    </div>
  )
}

export default function LlmPage() {
  return (
    <Suspense fallback={<NotFound />}>
      <LlmContent />
    </Suspense>
  )
}
